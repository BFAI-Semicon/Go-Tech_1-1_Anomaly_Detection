# Product Overview

ML実験プラットフォーム「LeadersBoard」は、外部からの投稿（コード/データ）を受け付け、サーバー側でGPU学習・評価を実行し、MLflowで実験結果を可視化・比較するリーダーボード機能を提供します。anomalibを用いた異常検知モデルの評価を想定した最小構成から開始し、将来的にはEvalAI統合や大規模スケールへの拡張を見据えています。

## Core Capabilities

- **提出受付**: 認証済みユーザーからのコード/データのマルチパートアップロード受付
- **非同期ジョブ実行**: Redisキュー + GPUワーカーによる学習・評価の非同期実行
- **実験可視化**: MLflow Tracking Serverによるパラメータ・メトリクス・アーティファクトの記録と比較ビュー
- **進捗管理**: ジョブ状態・ログ・結果の取得API（`run_id` とMLflow UIリンクの返却）
- **リーダーボード**: MLflow UIの比較ビューを活用したランキング表示

## Target Use Cases

- **研究者**: 自分の手法をアップロードし、標準データセット（例: MVTec AD）で評価結果を確認
- **研究者**: 他の提出と比較してランキング上の位置を把握
- **管理者**: ジョブの実行状況（進捗、ログ、エラー）を監視
- **管理者**: MLflow UIで全実験のメトリクス・アーティファクトを一覧・比較

## Value Proposition

- **透明性と再現性**: MLflowによる実験記録で、パラメータ・メトリクス・アーティファクトを完全追跡
- **公平な評価**: 統一された環境（GPUコンテナ、anomalib）で全提出を評価
- **拡張性**: 依存逆転（Clean-lite設計）により、将来の差し替えコスト最小化（ファイルシステム→S3、Redis→RabbitMQ、SQLite→Postgres等）
- **最小構成から開始**: docker-compose単機構成でPoC可能、将来はKubernetes移行・オートスケール対応
